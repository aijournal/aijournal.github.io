<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="//purl.org/dc/elements/1.1/" xmlns:content="//purl.org/rss/1.0/modules/content/" xmlns:atom="//www.w3.org/2005/Atom" version="2.0" xmlns:media="//search.yahoo.com/mrss/"><channel><title><![CDATA[reinforcement learning - AI Journal]]></title><description><![CDATA[AI Research Simplified]]></description><link>https://aijournal.github.io//</link><image><url>https://aijournal.github.io//favicon.png</url><title>reinforcement learning - AI Journal</title><link>https://aijournal.github.io//</link></image><generator>Ghost 2.0</generator><lastBuildDate>Fri, 02 Nov 2018 05:16:27 GMT</lastBuildDate><atom:link href="https://aijournal.github.io//tag/reinforcement-learning/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Deep Reinforcement Learning for Playing Hard Exploration Games (DeepMind)]]></title><description><![CDATA[Deep RL for playing Hard Exploration Games (DeepMind)]]></description><link>https://aijournal.github.io//deep-reinforcement-learning-for-playing-hard-exploration-games-deepmind-2/</link><guid isPermaLink="false">5bdbd9c9135e12107c61f8dd</guid><category><![CDATA[reinforcement learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Fri, 02 Nov 2018 05:02:31 GMT</pubDate><media:content url="https://aijournal.github.io//content/images/2018/11/index.jpeg" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io//content/images/2018/11/index.jpeg" alt="Deep Reinforcement Learning for Playing Hard Exploration Games (DeepMind)"><p>Deep reinforcement learning methods traditionally struggle with tasks  where environment rewards are particularly sparse. In this video we  discuss two-stage method that overcomes these limitations by relying on  noisy, unaligned footage without access to such data. </p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/f5NVkSXWoN4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Paper: <a href="https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;event=video_description&amp;redir_token=lXDGC75BZ1zeZay8_l29T2p2_v98MTU0MTIyMTE3NUAxNTQxMTM0Nzc1&amp;v=f5NVkSXWoN4" rel="nofollow">https://arxiv.org/abs/1805.11592</a></p><p>Deep Learning book: <a href="https://www.youtube.com/redirect?q=https%3A%2F%2Famzn.to%2F2FTO77C&amp;event=video_description&amp;redir_token=lXDGC75BZ1zeZay8_l29T2p2_v98MTU0MTIyMTE3NUAxNTQxMTM0Nzc1&amp;v=f5NVkSXWoN4" rel="nofollow">https://amzn.to/2FTO77C</a></p><p>Connect with me on Twitter: <a href="https://www.youtube.com/redirect?q=https%3A%2F%2Ftwitter.com%2Faijournalyt&amp;event=video_description&amp;redir_token=lXDGC75BZ1zeZay8_l29T2p2_v98MTU0MTIyMTE3NUAxNTQxMTM0Nzc1&amp;v=f5NVkSXWoN4" rel="nofollow">https://twitter.com/aijournalyt</a></p><p>Please support me on Patreon : <a href="https://www.youtube.com/redirect?q=https%3A%2F%2Fpatreon.com%2Faijournal&amp;event=video_description&amp;redir_token=lXDGC75BZ1zeZay8_l29T2p2_v98MTU0MTIyMTE3NUAxNTQxMTM0Nzc1&amp;v=f5NVkSXWoN4" rel="nofollow">https://patreon.com/aijournal</a></p>]]></content:encoded></item><item><title><![CDATA[Dynamic Programming in Reinforcement Learning]]></title><description><![CDATA[Dynamic Programming in Reinforcement Learning simplified]]></description><link>https://aijournal.github.io//dynamic-programming-in-reinforcement-learning/</link><guid isPermaLink="false">5bdbd8ce135e12107c61f8d8</guid><category><![CDATA[reinforcement learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Fri, 02 Nov 2018 04:59:11 GMT</pubDate><media:content url="https://aijournal.github.io//content/images/2018/11/LIVE-IN-THE-SUNSHINE.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io//content/images/2018/11/LIVE-IN-THE-SUNSHINE.png" alt="Dynamic Programming in Reinforcement Learning"><p>This episode gives an insight into the one commonly used method in field of Reinforcement Learning, Dynamic Programming.  We discuss:</p><ul><li>Dynamic Programming: Introduction </li><li>Policy Evaluation</li><li>Policy Improvement</li><li>Policy Iteration </li><li>Value Iteration </li><li>Asynchronous Dynamic Programming </li></ul><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/fAGgZrGZRsc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Subscribe to channel: <a href="https://www.youtube.com/aijournal/?sub_confirmation=1" rel="nofollow">https://www.youtube.com/aijournal/?su...</a> </p><p>Introduction to Reinforcement Learning : <a href="https://www.youtube.com/watch?v=-GkwGSYOW5A">https://www.youtube.com/watch?v=-GkwG...</a> </p><p>Code for this episode: <a href="https://www.youtube.com/redirect?event=video_description&amp;v=fAGgZrGZRsc&amp;redir_token=hB-Qdvh0IppKQ9_RKlECNRJ6SMd8MTU0MTIyMDc5OEAxNTQxMTM0Mzk4&amp;q=https%3A%2F%2Fgithub.com%2Fprajjwal1%2Freinforcement%2F" rel="nofollow">https://github.com/prajjwal1/reinforc...</a> </p><p>References: Reinforcement Learning by Sutton et al. ~ </p><p>Connect with me on Twitter: <a href="https://www.youtube.com/redirect?event=video_description&amp;v=fAGgZrGZRsc&amp;redir_token=hB-Qdvh0IppKQ9_RKlECNRJ6SMd8MTU0MTIyMDc5OEAxNTQxMTM0Mzk4&amp;q=https%3A%2F%2Ftwitter.com%2Faijournalyt" rel="nofollow">https://twitter.com/aijournalyt</a></p><p>Please support me on Patreon : <a href="https://www.youtube.com/redirect?event=video_description&amp;v=fAGgZrGZRsc&amp;redir_token=hB-Qdvh0IppKQ9_RKlECNRJ6SMd8MTU0MTIyMDc5OEAxNTQxMTM0Mzk4&amp;q=https%3A%2F%2Fpatreon.com%2Faijournal" rel="nofollow">https://patreon.com/aijournal</a></p>]]></content:encoded></item><item><title><![CDATA[Reinforcement Learning in a Nutshell]]></title><description><![CDATA[Intro to Reinforcement Learning ]]></description><link>https://aijournal.github.io//reinforcement-learning-in-a-nutshell/</link><guid isPermaLink="false">5bdbd7f3135e12107c61f8d6</guid><category><![CDATA[reinforcement learning]]></category><category><![CDATA[deep learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Fri, 02 Nov 2018 04:55:21 GMT</pubDate><media:content url="https://aijournal.github.io//content/images/2018/11/giphy-1.gif" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io//content/images/2018/11/giphy-1.gif" alt="Reinforcement Learning in a Nutshell"><p>Welcome to the first episode of Reinforcement Learning in a nutshell  where we discuss all the fundamentals of RL literature. We discuss  everything from agent to Markov Decision Processes to Optimal Policy  behaviour.</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/-GkwGSYOW5A?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p></p><p>Link to <a href="https://www.youtube.com/watch?v=fAGgZrGZRsc">Second Episode</a></p><p>Connect with me on Twitter: <a href="https://www.youtube.com/redirect?redir_token=OF2zbHyomZGX4hevUInjtCHyom58MTU0MTIyMDczN0AxNTQxMTM0MzM3&amp;event=video_description&amp;v=-GkwGSYOW5A&amp;q=https%3A%2F%2Ftwitter.com%2Faijournalyt" rel="nofollow">https://twitter.com/aijournalyt</a> </p><p>Please support me on Patreon : <a href="https://www.youtube.com/redirect?redir_token=OF2zbHyomZGX4hevUInjtCHyom58MTU0MTIyMDczN0AxNTQxMTM0MzM3&amp;event=video_description&amp;v=-GkwGSYOW5A&amp;q=https%3A%2F%2Fpatreon.com%2Faijournal" rel="nofollow">https://patreon.com/aijournal</a></p>]]></content:encoded></item></channel></rss>