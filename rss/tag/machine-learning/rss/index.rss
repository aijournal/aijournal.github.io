<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="//purl.org/dc/elements/1.1/" xmlns:content="//purl.org/rss/1.0/modules/content/" xmlns:atom="//www.w3.org/2005/Atom" version="2.0" xmlns:media="//search.yahoo.com/mrss/"><channel><title><![CDATA[deep learning - AI Journal]]></title><description><![CDATA[AI Research Simplified]]></description><link>https://aijournal.github.io/</link><image><url>https://aijournal.github.io/favicon.png</url><title>deep learning - AI Journal</title><link>https://aijournal.github.io/</link></image><generator>Ghost 2.0</generator><lastBuildDate>Mon, 17 Sep 2018 19:29:32 GMT</lastBuildDate><atom:link href="https://aijournal.github.io/tag/machine-learning/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Deep Learning Indaba 2018]]></title><description><![CDATA[Diversity in AI]]></description><link>https://aijournal.github.io/deep-learning-indaba/</link><guid isPermaLink="false">5b9fefb0c01296564869c102</guid><category><![CDATA[deep learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Mon, 17 Sep 2018 18:43:51 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-12-09-1.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-12-09-1.png" alt="Deep Learning Indaba 2018"><p> Deep Learning Indaba is a gathering to strengthen Machine Learning in Africa. We still lack diversity in AI and is currently being practiced in only certain regions actively. This is one of many initiatives to bring more diversity to AI. More than 500 students and researchers from Africa and around the world visited this event.</p><p>Such initiatives greatly help in fostering local ML communities and making research knowledge more accessible. People are more exposed to cutting edge research and it gives them opportunities to visit conferences like NIPS (travel grant). Here is the <a href="https://medium.com/@alienelf/an-open-letter-to-the-deep-learning-indaba-team-2723ec0ae0a8?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">open letter </a>to the Indaba Team which highlights the achievement of this initiative.</p><p>I truly believe that the benefits of AI can truly be realized only when it's not limited to some corporations/communities but when it's made publicly accessible (democratization). This is the only way to maximize the positive impact of AI.</p><p>This event was organized great set of researchers which includes Nando De Freitas (DeepMind), Jeff Dean (Chief of Google Brain), David Silver (DeepMind), Shakir Mohamed(DeepMind), Kyunghyun Cho (NYU) et al. as speakers</p><p>Jeff Dean talked greatly about exposing oneself to more diverse research. As per him, it's better to read more papers and gather more wider knowledge and inspiration rather than reading one paper in depth. </p><p>David Silver gave a talk on 10 Principles for Deep Reinforcement Learning. Sebastian Ruder has provided a nice summary of each slide in his this <a href="https://twitter.com/seb_ruder/status/1040235236284669952?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">thread </a>.</p><p>His talk revolved around how our RL agent should scale and this can be applied to common ML as well. </p><p>A list of Practicals also have been provided, <a href="https://github.com/deep-learning-indaba/indaba-2018?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Here</a> . You can directly run them on Google Cloud which is super convenient. </p><p><strong>How to write a great research paper  ?</strong></p><p>Nando  de Freitas, Ulrich Paquet, Stephan Gouws, Martin Arjovsky, and  Kyunghyun Cho gave a session on how to How to write a great research  paper. They discussed <a href="https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/?utm_campaign=NLP%20News&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">Simon Peyton Jones’ 7 simple suggestions</a>:</p><ul><li> Don’t wait: write.</li><li>Identify your key idea</li><li> Tell one story.</li><li>Nail your contributions to the mast.</li><li>Related work: later.</li><li>Put your readers first.</li><li>Listen to your readers.</li></ul><p>Here are some photos of the events</p><figure class="kg-image-card"><img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-09-45.png" class="kg-image" alt="Deep Learning Indaba 2018"><figcaption>Jeff Dean giving his talk</figcaption></figure><figure class="kg-image-card"><img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-10-17.png" class="kg-image" alt="Deep Learning Indaba 2018"></figure><figure class="kg-image-card"><img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-11-56.png" class="kg-image" alt="Deep Learning Indaba 2018"></figure><figure class="kg-image-card"><img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-12-09.png" class="kg-image" alt="Deep Learning Indaba 2018"></figure><figure class="kg-image-card"><img src="https://aijournal.github.io/content/images/2018/09/Screenshot-from-2018-09-18-00-12-37.png" class="kg-image" alt="Deep Learning Indaba 2018"></figure><blockquote><em>If you liked this, you may also like </em><a href="https://www.youtube.com/c/aijournal" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em>AI Journal</em></em></a><em><em> . Don’t forget to</em></em><a href="https://www.youtube.com/c/aijournal?sub_confirmation=1" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em> subscribe</em></em></a><em><em> . Stay connected with us on </em></em><a href="https://twitter.com/aijournalyt" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em>Twitter</em></em></a><em><em> to stay updated in AI Research. Please support me on </em></em><a href="https://www.patreon.com/aijournal" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener"><em><em>Patreon</em></em></a></blockquote>]]></content:encoded></item><item><title><![CDATA[This AI can dance | Everybody Dance Now Explained]]></title><description><![CDATA[Everybody dance now]]></description><link>https://aijournal.github.io/this-ai-can-dance/</link><guid isPermaLink="false">5b8ccea4ac8ef03bd64d8e61</guid><category><![CDATA[deep learning]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[Research]]></category><category><![CDATA[GANs]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Mon, 03 Sep 2018 06:07:37 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/09/thumb.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/09/thumb.png" alt="This AI can dance | Everybody Dance Now Explained"><p>This is the latest research work from berkeley ai research group. It is named as "Everybody dance now" because it presents "do as i do" motion transfer approach, so what does that mean in a nutshell? If we have a source video of a person, like in this one, this person is dancing at this moment. After a few minutes of the target subject performing these standard moves, we can transfer these moves to a another target . This can be treated as as a per-frame image-to-image translation with usage of spatio-temporal smoothing. We can use these pose detections as an intermediate representation between source and target therby learning a mapping from pose images of the source to the target subject’s appearance. So we can perform transfer of motion between human subjects in different videos, we have source dancing in some place, and we can use those pose estimations and bring that to different target subjects.</p><p>Have a look at this for more details:</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/vya2wOk26GA?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><p>Link to research paper [<a href="https://arxiv.org/abs/1808.07371">arXiv</a>]</p><blockquote><em><em>Visit </em></em><a href="https://www.youtube.com/c/aijournal" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em>AI Journal</em></em></a><em><em> for more videos. Don’t forget to</em></em><a href="https://www.youtube.com/c/aijournal?sub_confirmation=1" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em> subscribe</em></em></a><em><em> . Stay connected with us on </em></em><a href="https://twitter.com/aijournalyt" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em><em>Twitter</em></em></a><em><em> to stay updated in AI Research. Please support me on </em></em><a href="https://www.patreon.com/aijournal" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener"><em><em>Patreon</em></em></a></blockquote>]]></content:encoded></item><item><title><![CDATA[Incremental Learning in Deep Learning]]></title><description><![CDATA[Transfer Learning is the next frontier !]]></description><link>https://aijournal.github.io/incremental-learning-in-deep-learning/</link><guid isPermaLink="false">5b858912cb504a197ab751ed</guid><category><![CDATA[deep learning]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[Research]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Tue, 28 Aug 2018 17:42:44 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/08/a1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/08/a1.jpg" alt="Incremental Learning in Deep Learning"><p><strong>Abstract</strong></p><p>Researchers  often try to capture as much information as they can, either by using  existing architectures, creating new ones, going deeper, or employing  different training methods. This paper compares different ideas and  methods that are used heavily in Machine Learning to determine what  works best. These methods are prevalent in various domains of Machine  Learning, such as Computer Vision and Natural Language Processing (NLP).</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/q4A76i6TOCc?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><h3 id="transfer-learning-is-the-key">Transfer Learning is the Key</h3><p>Throughout  our work, we have tried to bring generalization into context, because  that’s what matters in the end. Any model should be robust and able to  work outside your research environment. When a model lacks  generalization, very often we try to train the model on datasets it has  never encountered … and that’s when things start to get much more  complex. Each dataset comes with its own added features which we have to  adjust to accommodate our model.</p><p>One common way to do so is to transfer learning from one domain to another.</p><p>Given  a specific task in a particular domain, for which we need labelled  images for the same task and domain, we train our model on that dataset.  In practice, the dataset is usually the largest in that domain so that  we can leverage the features extracted effectively. In computer vision,  it’s mostly Imagenet, which has 1,000 classes and more than 1 million  images. When training your network upon it, it’s bound to extract  features2 that are difficult to obtain otherwise. Initial layers usually  capture small, fine details, and as we go deeper, ConvNets try to  capture task-specific details; this makes ConvNets fantastic feature  extractors.</p><p>Normally  we let ConvNet capture features by training it on a larger dataset and  then modify. Fully connected layers in the end can do whatever we  require for carrying out classification, and we can add a combination of  linear layers. This makes it easy to transfer the knowledge of our  network to carry out another task.</p><h4 id="to-read-more-about-it-please-refer-to-this-original-paper-">To read more about it, please refer to this original paper:</h4><h4 id="using-transfer-learning-to-introduce-generalization-in-models"><a href="https://software.intel.com/en-us/articles/part-1-using-transfer-learning-to-introduce-generalization-in-models" rel="nofollow noopener">Using Transfer Learning to Introduce Generalization in Models</a></h4><p></p><h4 id="also-transfer-learning-in-nlp-is-out-now-">Also Transfer Learning in NLP is out now:</h4><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/zxJJ0T54HX8?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><blockquote><em>Visit </em><a href="https://www.youtube.com/c/aijournal" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em>AI Journal</em></a><em> for more videos. Don’t forget to</em><a href="https://www.youtube.com/c/aijournal?sub_confirmation=1" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em> subscribe</em></a><em> . Stay connected with us on </em><a href="https://twitter.com/aijournalyt" rel="noopener nofollow nofollow noopener nofollow noopener nofollow noopener"><em>Twitter</em></a><em> to stay updated in AI Research. Please support me on </em><a href="https://www.patreon.com/aijournal" rel="nofollow noopener nofollow noopener nofollow noopener nofollow noopener"><em>Patreon</em></a></blockquote><h3 id="link-to-all-research-papers-">Link to all research papers:</h3><ol><li><a href="https://arxiv.org/abs/1312.6211" rel="nofollow noopener">An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks</a></li><li><a href="https://arxiv.org/abs/1311.2901" rel="nofollow noopener">Visualizing and Understanding Convolutional Networks</a></li><li><a href="https://arxiv.org/abs/1801.06146" rel="nofollow noopener">Universal Language Model Fine-tuning for Text Classification</a></li><li><a href="https://arxiv.org/abs/1606.09282" rel="nofollow noopener">Learning Without Forgetting</a></li><li><a href="https://arxiv.org/abs/1512.03385" rel="nofollow noopener">Deep Residual Learning for Image Recognition</a></li><li><a href="https://arxiv.org/abs/1801.04016" rel="nofollow noopener">Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution</a></li></ol>]]></content:encoded></item><item><title><![CDATA[Everything you need to know about Recurrent Neural Networks]]></title><description><![CDATA[Recurrent Neural Networks are really powerful . They have memory and can deal with long term dependencies.]]></description><link>https://aijournal.github.io/everything-you-need-to-know-about-recurrent-neural-networks/</link><guid isPermaLink="false">5b858897cb504a197ab751e8</guid><category><![CDATA[deep learning]]></category><category><![CDATA[NLP]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Tue, 28 Aug 2018 17:40:03 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/08/rvb.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/08/rvb.png" alt="Everything you need to know about Recurrent Neural Networks"><p>Recurrent Neural Networks are really powerful . They have memory and can deal with long term dependencies.</p><p>In this video, we deep dive into:</p><ul><li>What Recurrent Neural Networks are?</li><li>How are they different from other Neural Network architecture ?</li><li>How do RNNs work ?</li><li>Long short term memory networks(LSTMs)</li><li>Mathematical formulations of RNNs/LSTMs</li><li>Gated Recurrent Units (GRUs)</li></ul><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/4tlrXYBt50s?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure>]]></content:encoded></item><item><title><![CDATA[How to use Kaggle ?]]></title><description><![CDATA[Kaggle Tutorial]]></description><link>https://aijournal.github.io/how-to-use-kaggle/</link><guid isPermaLink="false">5b85880ccb504a197ab751e2</guid><category><![CDATA[deep learning]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Tue, 28 Aug 2018 17:37:26 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/08/kg.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/08/kg.png" alt="How to use Kaggle ?"><p>Kaggle  is a platform for predictive modelling and analytics competitions in  which statisticians and data miners compete to produce the best models  for predicting and describing the datasets uploaded by companies and  users.</p><p>Kaggle  let’s you participate in Machine Learning/Data Science competitions  that not only improves your rank but is an awesome way to boost your  skills. There is a steep learning curve in every competition that you  participate in in order to make it to top of leaderboard.</p><p>But  it’s more than that, it has kernels that you can use to work on models  that someone already has worked upon or create your own to help others.  It is an online interactive jupyter environment with few tweaks. Cool  huh !</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/Gp_qv317Gew?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure>]]></content:encoded></item><item><title><![CDATA[Attention and memory in Deep Learning]]></title><description><![CDATA[Attention and Memory in Deep Learning]]></description><link>https://aijournal.github.io/attention-and-memory-in-deep-learning/</link><guid isPermaLink="false">5b85870ecb504a197ab751de</guid><category><![CDATA[deep learning]]></category><category><![CDATA[NLP]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Tue, 28 Aug 2018 17:35:29 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/08/att.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/08/att.png" alt="Attention and memory in Deep Learning"><p>Pretty much all the stuff has been explained in this video:</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/H9NVFiqLLFU?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><p>Some prerequisites for better understanding</p><p>To learn more about LSTM/GRU/Recurrent Neural Networks:</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/4tlrXYBt50s?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><p>To know more about Sequence to Sequence (Seq2Seq) models</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/oF0Rboc4IJw?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><p>Research papers discussed in Attention and memory in Deep Learning:</p><ul><li><a href="//papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine" rel="nofollow noopener">Learning to combine foveal glimpses with a third-order Boltzmann machine</a></li><li><a href="//arxiv.org/abs/1109.3737" rel="nofollow noopener">Learning where to Attend with Deep Architectures for Image Tracking</a>.</li><li><a href="https://arxiv.org/abs/1409.3215" rel="nofollow noopener"><strong>Sequence to Sequence Learning with Neural Networks</strong></a></li><li><a href="https://arxiv.org/abs/1410.4615" rel="nofollow noopener">Learning to Execute</a></li><li><a href="//arxiv.org/abs/1409.0473" rel="nofollow noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="//arxiv.org/abs/1406.6247" rel="nofollow noopener">Recurrent Models of Visual Attention</a></li></ul><p>Examples discussed:</p><ul><li><a href="//arxiv.org/abs/1502.03044" rel="nofollow noopener">Show, Attend and Tell</a></li><li><a href="//arxiv.org/abs/1412.7449" rel="nofollow noopener">Grammar as a Foreign Language</a></li><li><a href="//arxiv.org/abs/1506.03340" rel="nofollow noopener">Teaching Machines to Read and Comprehend</a></li></ul><p>Some more papers:</p><ul><li><a href="//arxiv.org/abs/1503.08895" rel="nofollow noopener">End-to-End Memory Networks</a></li><li><a href="https://arxiv.org/abs/1505.00521" rel="nofollow noopener">Reinforcement Learning Neural Turing Machines — Revised</a></li></ul><blockquote><em>Visit </em><a href="https://www.youtube.com/c/aijournal" rel="noopener nofollow nofollow noopener"><em>AI Journal</em></a><em> for more videos. Don’t forget to</em><a href="https://www.youtube.com/c/aijournal?sub_confirmation=1" rel="noopener nofollow nofollow noopener"><em> subscribe</em></a><em> . Stay connected with us on </em><a href="https://twitter.com/aijournalyt" rel="noopener nofollow nofollow noopener"><em>Twitter</em></a><em> to stay updated in AI Research. Please support me on </em><a href="https://www.patreon.com/aijournal" rel="nofollow noopener nofollow noopener"><em>Patreon</em></a></blockquote>]]></content:encoded></item><item><title><![CDATA[Deep Reinforcement Learning for Playing Hard Exploration Games (DeepMind)]]></title><description><![CDATA[Deep reinforcement learning methods traditionally struggle with tasks  where environment rewards are particularly sparse]]></description><link>https://aijournal.github.io/deep-reinforcement-learning-for-playing-hard-exploration-games-deepmind/</link><guid isPermaLink="false">5b858689cb504a197ab751d9</guid><category><![CDATA[deep learning]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[reinforcement learning]]></category><dc:creator><![CDATA[AI Journal]]></dc:creator><pubDate>Tue, 28 Aug 2018 17:31:34 GMT</pubDate><media:content url="https://aijournal.github.io/content/images/2018/08/deepmind_cover.png" medium="image"/><content:encoded><![CDATA[<img src="https://aijournal.github.io/content/images/2018/08/deepmind_cover.png" alt="Deep Reinforcement Learning for Playing Hard Exploration Games (DeepMind)"><p>Deep reinforcement learning methods traditionally struggle with tasks  where environment rewards are particularly sparse. In this video we  discuss two-stage method that overcomes these limitations by relying on  noisy, unaligned footage without access to such data</p><figure class="kg-embed-card"><iframe width="480" height="270" src="https://www.youtube.com/embed/f5NVkSXWoN4?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></figure><p>Read the Paper here : <a href="https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fabs%2F1805.11592&amp;redir_token=j6RNw3q9Y6W2vSjw8aUAQLC0O3F8MTUyODc4MjUxMUAxNTI4Njk2MTEx&amp;v=f5NVkSXWoN4&amp;event=video_description" rel="nofollow noopener">https://arxiv.org/abs/1805.11592</a></p><blockquote>Visit <a href="https://www.youtube.com/c/aijournal" rel="noopener nofollow nofollow noopener nofollow noopener">AI Journal</a> for more videos. Don’t forget to<a href="https://www.youtube.com/c/aijournal?sub_confirmation=1" rel="noopener nofollow nofollow noopener nofollow noopener"> subscribe</a> . Stay connected with us on <a href="https://twitter.com/aijournalyt" rel="noopener nofollow nofollow noopener nofollow noopener">Twitter</a> to stay updated in AI Research. Please support me on <a href="https://www.patreon.com/aijournal" rel="nofollow noopener nofollow noopener nofollow noopener">Patreon</a></blockquote>]]></content:encoded></item></channel></rss>